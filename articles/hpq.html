<!DOCTYPE html>
<html lang = "en">

  <head>
    <title>Oscar Ryley</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="..\style.css">
    <meta charset="UTF-8">
    <meta name="description" content="Oscar's Essay Content">
    <meta name="keywords" content="Oscar">
    <meta name="author" content="Oscar Ryley">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  </head>

  <body>

    <nav class="navbar navbar-expand navbar-light" style="background-color: rgba(0, 0, 0, 0.025); margin-bottom: -1.5em;">
      <div class="container-fluid">
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav me-auto mb-2 mb-lg-0" style="margin-bottom: -1em;">
            <li class="nav-item">
              <a class="nav-link active" aria-current="page" href="../index.html"><h4><b>&nbsp; Home &nbsp;</b></h4></a>
            </li>
            <li class="nav-item">
                <a class="nav-link active" aria-current="page" href="../experience.html"><h4>&nbsp; Experience &nbsp;</h4></a>
            </li>
            <li class="nav-item">
              <a class="nav-link active" aria-current="page" href="../articles.html"><h4>&nbsp; Articles &nbsp;</h4></a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <div class="mt-4 p-5 text-center" style="background-color: rgba(0, 0, 0, 0.025);">
      <h1 class="display-2 fw-bold"><b>Should your Car be allowed to Kill You?</b></h1>
      <p class="fs-4">An analysis into the ethics behind what an automated vehicle should morally do in the time of an emergency.</p>
      <p class="text-muted" style="margin-bottom: -0.2em;" ><small>Written by Oscar Ryley &nbsp; <a style="color:lightgrey" href="https://www.linkedin.com/in/oscar-ryley/" target="_blank">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-linkedin" viewBox="0 0 16 16"> 
          <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"/>
        </svg></a></small></p>
      <p class="text-muted" style="margin-bottom: -1em;"><small>essay published 26/06/2020</small></p>
    </div>

    <br>
    <div class="container">
      <div class="row">
        <div class="col-md-1"></div>
        <div class="col">
          
          <p id="article-text">
            This essay goes into depth about the very current moral issue about what a self-driving car
            should do in the case of an accident, it discusses the different moral viewpoints that could be
            applied to this ethical dilemma.
            <br><br>
            The main issue that this essay will be going into is the issue put forward by
            'moralmachine.mit.edu' and many other modern day philosophers, what should a
            self-driving car do if its only options were to crash into one group of people or another, how
            would it make the decision, would it base it off of its interpretation of the value of life and
            should it even act at all. I believe that we can use the different interpretations of moral
            philosophy put forward by philosophers to answer these questions and thoroughly delve into
            the philosophy of right and wrong.
            <br><br>
            A moral viewpoint is a perspective (most commonly a set of rules) that describes what makes
            an act right or wrong. Throughout this essay I will be describing and comparing these points
            of view as well as applying their 'rules' to the self-driving car moral dilemma that is akin to
            the trolley problem. These moral viewpoints take into account the consequences and
            intentions of people's actions and talk about what to do and whether or not to act in moral
            dilemmas. The question of what is right and what is wrong is one that has troubled
            philosophers throughout the ages and the following moral viewpoints give an opinion on
            this unsolvable question; through the examination of these viewpoints this essay will try to
            solve the ethical problem with self-driving car's AI.
            <br><br>
            According to the Stanford Encyclopedia of Philosophy, Ethical Egoism is the belief that each
            person has only one ultimate aim: their ultimate welfare. When making decisions using this
            moral position the morally right option is the one that benefits the one doing the action the
            most. This means that it is very different from other moral theories as it does not demand
            the user to weigh up the interests of others when making morally good decisions.
            <br><br>
            In the case of self-driving cars having to pick between two groups of people an ethical egoist
            would say that the morally right thing to do would be the option that saves the life of either
            the person driving the car or the option that stops the car being damaged. However, the
            latter primary aim of saving the car over human people goes against Issac Asimov's first and
            third laws of robotics that he first outlined in his short story 'Runaround' from his short
            story collection 'I, Robot' published in 1950. These rules describe how a robot may not injure
            a human being and may only protect its own existence if it doesn't mean that it will have to
            harm a human being and even though they are a work of fiction I believe that they are a good
            foundation for the rules that we should apply to AI. Another problem that arises with a
            self-driving car always saving the passengers when faced with an emergency is the issue of
            how far the car will go to save its occupants. This is covered in a John Finnemore sketch from
            the second episode of the sixth series of his show 'John Finnemore's Souvenir programme.'
            <br><br>
            Golden Rule ethics is the belief that all moral decisions that people make should be based on
            the one 'Golden Rule' and that straying from this one rule is morally wrong. The rule,
            according to Britannica is the fundamental ethical principle from the bible (Matthew 7:12)
            “In everything, do to others what you would have them do to you” that states that a Christian
            must live their life by the principle that they should be kind to people if they expect kindness
            in return. Another ethical view that is similar to this in that it ties religion to what is right
            and wrong is Divine Command theory which, according to the article on Internet
            Encyclopedia of Philosophy by Austin, M, states that morality is ultimately based on what a
            deity or God commands is right and wrong. Even Immanuel Kant, a philosopher with his
            own views on morality that will be covered later, says that morality requires belief in a God
            because 'the requirements of morality are too much for us to bear.'
            <br><br>
            Whilst a self-driving car couldn't follow a religion or base its actions on religious preaching it
            could still follow a 'Golden Rule' when deciding what to do and who to save in an accident. If
            a self-driving car were to follow the Golden Rule in response to this trolley problem like
            dilemma it would have to somehow be kind to the humans in this situation like by being
            selfless for the people in the car and saving the people who aren't in the car.
            <br><br>
            Situation ethics was created by Joseph F. Fletcher in his 1966 book 'Situation Ethics: The New
            Morality' and was in opposition to moral theories that relied on an absolutionalist view that
            there are fixed universally moral principles. Situation ethics, according to the Britannica
            article of the same name by Rosenthal, S et al, is the ethical position that states that moral
            decision judgements must be made within the context of the entire situation and be based
            on a guiding framework of acting in a loving way.
            <br><br>
            When applied to the self-driving car ethical dilemma a Situation ethicist would look at the
            specifics of the situation, including the details of the different people that the car could save
            or inadvertently kill. The self-driving car would also have to abide by the general christian
            norm of brotherly love that the creator of situation ethics Joseph F. Fletcher said could be
            expressed in different ways in different situations. For a self-driving car's AI to act in a
            different way according to whatever situation that it was in, it would have to take all of the
            factors into account and weigh them against each other to come to a final decision whilst
            still caring about all of the people affected.
            <br><br>
            As Green, H, teaches in the 35th instalment of his series “Crash Course Philosophy”
            Immanuel Kant was an eighteenth century German Philosopher who disliked the idea of
            tying religion to morality and instead said that morality was a fixed law of nature. This
            makes the Kantian view on morality an absolutist philosophical viewpoint (one that follows a
            direct set of rules to get the answer on what is right and what is wrong). The rules that Kant
            theorised about are called Categorical Imperatives and they are a set of moral obligations
            that you have to follow. These imperatives were derived from pure reason according to Kant;
            he believed that they “were totally knowable just by using your intellect” and therefore did
            not require religion to guide people into knowing right from wrong.
            <br><br>
            Kant's first categorical imperative is the Universalizability maxim, which states that if you
            do an action and you see it as morally right then that action should be always morally right-
            you have to follow universal laws in everything that you do. According to this imperative, as
            long as you are following these universal moral rules then any consequence of your actions is
            not your responsibility. You have to follow all moral rules, even if they might result in bad
            things happening. This view on responsibility is very interesting when talking about self
            driving cars.
            <br><br>
            A self driving car that followed this first categorical imperative would have to follow all
            universal laws (only do things that humans, through logic, say are always right). This means
            that an autonomous vehicle would, when faced with the decision of moving to save people's
            lives over other people's lives, have to follow the logical universal law that killing someone is
            wrong and not act, killing the group of people that it was going to without input from the
            car. Kantism would also argue that the death of the group of people in the car's path because
            of the self-driving car's inaction are not the fault of the manufacturers or the car itself
            because no responsibility should be placed on those who follow the universal laws- the
            consequences of their actions are not their fault. So a Kantist self-driving car would not act
            in times of emergency.
            <br><br>
            The exact opposite of Kant's view on moral philosophy was thought up by two other 18th
            century philosophers, who lived in Great Britain, Jeremy Benthem and John Stuart Mill.
            Utilitarianism is a moral position that has been developed throughout the history of moral
            philosophy and, according to the Stanford Encyclopedia of Philosophy, it states that an
            action that is morally right is one that does the greatest good for the highest number of
            people. In situations where you are weighing up the happiness brought about by two
            different options everyone's happiness is weighed equally and without any bias, this makes
            Utilitarianism impartial and agent-neutral. “Crash Course Philosophy”'s 36th episode goes
            further in depth about this happiness that you are comparing; This happiness that you weigh
            up is defined further as pleasure because this is the main goal of every human endeavour- we
            do everything that we do in pursuit of pleasure so that should be what shapes our moral
            philosophy. This moral viewpoint is the polar opposite to Kantiansim because it looks solely
            at the consequences of people's actions, takes into account what happens through someone's
            inaction as well as action and doesn't take intent into account.
            <br><br>
            There are two different branches of Utilitarianism; these are Act and Rule Utilitarianism. The
            former, Act Utilitarianism, is the classical view first proposed by Jeremy Benthem and John
            Stuart Mill that suggests that you have to act in a way that is benefiting the most people in all
            situations- even if it means making personal sacrifices. This is the strand of Utilitarianism
            that gets the most opposition, including the criticism known as the repugnant conclusion
            which states that Act Utilitarianism would claim that fifty-one people with happiness at a
            level of 1/10 is better than five people with happiness at a level of 10/10.
            <br><br>
            However, the other form of Utilitarianism, Rule Utilitarianism states that sometimes the act
            that does the most good can be morally wrong, like in the repugnant conclusion. This means
            that it is morally right to follow rules that will lead, in most cases, to the greatest good for the
            greatest number. Rule Utilitarianism focuses on acts that maximise utility in the long term
            instead of just thinking about actions on a case by case basis.
            <br><br>
            When applied to self-driving cars I think that in most cases both strands of Utilitarianism
            would choose the option that would maximise happiness for all those involved- in most
            situations this would be the option that saves the most lives because Utilitarianism orders
            that everyone's happiness would be seen as equal- however, the happiness of loved ones
            would have to be put into account when deciding who to save the lives of.
            <br><br>
            When deciding what a self-driving car should do in the time of an emergency I think, after
            all the research that I have done into the topic of what philosophers have said about the
            eternal 'what is right and wrong' question throughout history, that the best moral viewpoint
            to base the car's decision making AI on would be Utilitarianism because it thinks most about
            the specifics of the consequences of what the car decides to do and has the most impartial
            view on what to do during a crash. This means that when a self-driving car could choose
            between two sets of people's lives it should save the group of people that would affect the
            largest group of people if they died. This would require a lot of data collection on everyone
            involved which is an ethical dilemma for another day, but making the morally right decision
            isn't easy- as illustrated by the many different opinions held by philosophers throughout
            philosophical history.
          </p>
        </div>
        <div class="col-md-1"></div>
      </div>
    </div>

    <br>
    <hr style="color:lightgrey;">
    <br>

    <div class="container">
      <div class="row">
        <div class="col-md-1"></div>
        <div class="col">
          <h1>Bibliography and Further Reading</h1>
          <ul id="article-text">
            <li>Rooney, A, (2019) Think like a philosopher- get to grips with reasoning and ethics, Arturus</li>
            <li>Rahwan, I + Bonnefon, J-F + Shariff, A, et al , (2018) Moral Machine Website <a href="http://moralmachine.mit.edu/" target="_blank">http://moralmachine.mit.edu/</a>, [date accessed 2/2/2020]</li>
            <li>Finnemore, J, (14 July 2019) <a href="https://www.youtube.com/watch?v=ejcih7N_gnY" target="_blank">https://www.youtube.com/watch?v=ejcih7N_gnY</a>, [date accessed 2/2/2020]</li>
            <li>Asimov, I, (1950) 'Runaround' from 'I, Robot', Gnome press</li>
            <li>Driver, J, (March 27, 2009) Stanford Encyclopedia of Philosophy - The History of Utilitarianism, <a href="https://plato.stanford.edu/entries/utilitarianism-history/" target="_blank">https://plato.stanford.edu/entries/utilitarianism-history/</a> , [date accessed 16/3/2020]</li>
            <li>Hooker, B, (December 31, 2003) Stanford Encyclopedia of Philosophy - Rule Consequentialism, <a href="https://plato.stanford.edu/entries/consequentialism-rule/" target="_blank">https://plato.stanford.edu/entries/consequentialism-rule/</a>, [date accessed 16/3/2020]</li>
            <li>Wilson, E + Denis, L, (March 26, 2008) Stanford Encyclopedia of Philosophy - Kant and Hume on Moraliy <a href="https://plato.stanford.edu/entries/kant-hume-morality/" target="_blank">https://plato.stanford.edu/entries/kant-hume-morality/</a>, [date accessed 16/3/2020]</li>
            <li>Shaver, R, (March 21, 2019) Stanford Encyclopedia of Philosophy- Egoism <a href="https://plato.stanford.edu/entries/egoism/" target="_blank">https://plato.stanford.edu/entries/egoism/</a>, [date accessed 18/4/2020]</li>
            <li>Jean, H et al, (March 10, 2016) Kantianism philosophy <a href="https://www.britannica.com/topic/Kantianism" target="_blank">https://www.britannica.com/topic/Kantianism</a>, [date accessed 18/4/2020]</li>
            <li>Rosenthal, S et al, (January 8, 2019) Situation ethics <a href="https://www.britannica.com/topic/situation-ethics" target="_blank">https://www.britannica.com/topic/situation-ethics</a>, [date accessed 18/4/2020]</li>
            <li>Chopra, S et al, (January 15, 2012) Golden Rule - ethical precept https://www.britannica.com/topic/Golden-Rule, [date accessed 18/4/2020]</li>
            <li>Austin, M, () The Internet of Philosophy- Divine command theory <a href="https://www.iep.utm.edu/divine-c/" target="_blank">https://www.iep.utm.edu/divine-c/</a>, [date accessed 19/4/2020]</li>
            <li>Fletcher, J, (1966) Situation Ethics: The New Morality, Westminster John Knox Press</li>
            <li>Green, H, et al (14 November 2016) <a href="https://www.youtube.com/watch?v=8bIys6JoEDw" target="_blank">https://www.youtube.com/watch?v=8bIys6JoEDw</a>, [date accessed 21/4/2020]</li>
            <li>Green, H, et al (21 November 2016) <a href="https://www.youtube.com/watch?v=-a739VjqdSI" target="_blank">https://www.youtube.com/watch?v=-a739VjqdSI</a>, [date accessed 21/4/2020]</li>
          </ul>
        </div>
        <div class="col-md-1"></div>
      </div>
    </div>

    <br><br><br>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
  </body>

  <footer class="text-center bg-white text-muted" style="margin-bottom: -1em;">
    <div style="background-color: rgba(0, 0, 0, 0.025);">
      <p><small>AUTHORED BY OSCAR RYLEY</small> &nbsp; &nbsp;
        <a style="color:grey" href="https://www.linkedin.com/in/oscar-ryley/" target="_blank">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-linkedin" viewBox="0 0 16 16"> 
            <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"/>
          </svg></a> &nbsp; &nbsp;
        <a style="color:grey" href="https://github.com/Oscar-Ryley" target="_blank"> 
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-github" viewBox="0 0 16 16">
            <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/> 
          </svg></a>
      </p>
      <p style="margin-top: -1.25em;"><small>© 2024 Copyright: Oscar Ryley</small></p>
    </div>
  </footer>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
</html>